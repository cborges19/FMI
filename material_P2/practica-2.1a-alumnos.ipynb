{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58958819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b3f06",
   "metadata": {},
   "source": [
    "## 1. Cálculo de entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef68bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la entropía estimada a partir de las realizaciones\n",
    "def entropy_estimation(data):\n",
    "    unique_values, counts = np.unique(data, return_counts=True)\n",
    "    probabilities = counts / len(data)  # Probabilidades estimadas (frecuencias relativas)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))  # Cálculo de entropía\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71163f4f",
   "metadata": {},
   "source": [
    "### ENTROPIA DE UN TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff6d9f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía calculada: 4.2460 bits por símbolo\n",
      "Entropía calculada: 4.2460 bits por símbolo\n",
      "Entropía del texto (normalizado): 4.0015 bits/símbolo\n"
     ]
    }
   ],
   "source": [
    "texto2 = \"\"\"En un lugar de la Mancha2, de cuyo nombre no quiero acordarme3, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor4. Una olla de algo más vaca que carnero, salpicón las más noches5, duelos y quebrantos los sábados6, lantejas los viernes7, algún palomino de añadidura los domingos8, consumían las tres partes de su hacienda9. El resto della concluían sayo de velarte10, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo11, y los días de entresemana se honraba con su vellorí de lo más fino12. Tenía en su casa una ama que pasaba de los cuarenta y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza que así ensillaba el rocín como tomaba la podadera13. Frisaba la edad de nuestro hidalgo con los cincuenta años14. Era de complexión recia, seco de carnes, enjuto de rostro15, gran madrugador y amigo de la caza. Quieren decir que tenía el sobrenombre de «Quijada», o «Quesada», que en esto hay alguna diferencia en los autores que deste caso escriben, aunque por conjeturas verisímilesII se deja entender que se llamaba «Quijana»III, 16. Pero esto importa poco a nuestro cuento: basta que en la narración dél no se salga un punto de la verdad.\n",
    "\n",
    "Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba ocioso —que eran los más del año—, se daba a leer libros de caballerías, con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza y aun la administración de su hacienda; y llegó a tanto su curiosidad y desatino en esto17, que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en queIV leer18, y, así, llevó a su casa todos cuantos pudo haber dellos; y, de todos, ningunos le parecían tan bienV como los que compuso el famoso Feliciano de Silva19, porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas, y más cuando llegaba a leer aquellos requiebros y cartas de desafíos20, donde en muchas partes hallaba escrito: «La razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura»21. Y también cuando leía: «Los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican y os hacen merecedora del merecimiento que merece la vuestra grandeza...»22\n",
    "\n",
    "Con estas razones perdía el pobre caballero el juicio, y desvelábase por entenderlas y desentrañarles el sentido, que no se lo sacara ni las entendiera el mesmo Aristóteles, si resucitara para solo ello. No estaba muy bien con las heridas que don Belianís daba y recebía, porque se imaginaba que, por grandes maestros que le hubiesen curado, no dejaría de tener el rostro y todo el cuerpo lleno de cicatrices y señales23. Pero, con todo, alababa en su autor aquel acabar su libro con la promesa de aquella inacabable aventura, y muchas veces le vino deseo de tomar la pluma y dalle fin al pie de la letra como allí se promete24; y sin duda alguna lo hiciera, y aun saliera con ello25, si otros mayores y continuos pensamientos no se lo estorbaran. Tuvo muchas veces competencia con el cura de su lugar —que era hombre docto, graduado en Cigüenza—26 sobre cuál había sido mejor caballero: Palmerín de Ingalaterra o Amadís de Gaula27; mas maese Nicolás, barbero del mesmo pueblo28, decía que ninguno llegaba al Caballero del Febo, y que si alguno se le podía comparar era don Galaor, hermano de Amadís de Gaula, porque tenía muy acomodada condición para todo, que no era caballero melindroso, ni tan llorón como su hermano, y que en lo de la valentía no le iba en zaga29.\n",
    "\n",
    "En resolución, él se enfrascó tanto en su letura, que se le pasaban las noches leyendo de claro en claro30, y los días de turbio en turbio; y así, del poco dormir y del mucho leer, se le secó el celebro de manera que vino a perder el juicio31. Llenósele la fantasía de todo aquello que leía en los libros, así de encantamentos como de pendencias, batallas, desafíos, heridas, requiebros, amores, tormentas y disparates imposibles; y asentósele de tal modo en la imaginación que era verdad toda aquella máquina de aquellas soñadas invencionesVI que leía32, que para él no había otra historia más cierta en el mundo33. Decía él que el Cid Ruy Díaz había sido muy buen caballero, pero que no tenía que ver con el Caballero de la Ardiente Espada, que de solo un revés había partido por medio dos fieros y descomunales gigantes34. Mejor estaba con Bernardo del Carpio, porque en Roncesvalles había muerto a Roldán, el encantado35, valiéndose de la industria de Hércules, cuando ahogó a AnteoVII, el hijo de la Tierra, entre los brazos36. Decía mucho bien del gigante Morgante, porque, con ser de aquella generación gigantea, que todos son soberbios y descomedidos, él solo era afable y bien criado37. Pero, sobre todos, estaba bien con Reinaldos de Montalbán, y más cuando le veía salir de su castillo y robar cuantos topaba, y cuando en allende robó aquel ídolo de Mahoma que era todo de oro, según dice su historia38. Diera él, por dar una mano de coces al traidor de Galalón39, al ama que tenía, y aun a su sobrina de añadidura.\"\"\"\n",
    "\n",
    "\n",
    "# Opción A: Entropía de los caracteres directamente\n",
    "datos_caracteres = list(texto)\n",
    "e_caracteres = entropy_estimation(datos_caracteres)\n",
    "\n",
    "# Opción B: Entropía de los valores ASCII (numérico)\n",
    "datos_num = [ord(c) for c in texto]\n",
    "e_num = entropy_estimation(datos_num)\n",
    "\n",
    "print(f\"Entropía calculada: {e_caracteres:.4f} bits por símbolo\")\n",
    "print(f\"Entropía calculada: {e_num:.4f} bits por símbolo\")\n",
    "\n",
    "# 2. Preprocesamiento sin 're' (Limpieza manual)\n",
    "# Convertimos a minúsculas para unificar símbolos (ej: 'E' y 'e')\n",
    "texto_min = texto.lower()\n",
    "\n",
    "# Filtramos: solo conservamos letras (a-z) y espacios, eliminando números y puntuación\n",
    "# Esto actúa como un filtro para eliminar \"ruido\" impulsivo\n",
    "caracteres_filtrados = [c for c in texto_min if c.isalpha() or c.isspace()]\n",
    "\n",
    "# 3. Cálculo de la entropía\n",
    "resultado = entropy_estimation(caracteres_filtrados)\n",
    "\n",
    "print(f\"Entropía del texto (normalizado): {resultado:.4f} bits/símbolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e5ea7",
   "metadata": {},
   "source": [
    "### 1.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb6ea35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La entropía estimada H(X) es: 1.5219 bits\n",
      "La entropía estimada H(Y) es: 1.5219 bits\n"
     ]
    }
   ],
   "source": [
    "# Realizaciones de las variables aleatorias X e Y\n",
    "# (reemplazar estos valores con los datos reales de las realizaciones)\n",
    "X = [1, 2, 1, 3, 2, 2, 3, 1, 1, 2]  # Ejemplo de realizaciones de X\n",
    "Y = [2, 3, 2, 3, 1, 2, 3, 1, 3, 2]  # Ejemplo de realizaciones de Y\n",
    "realizaciones = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [1, 2],\n",
    "    [3, 3],\n",
    "    [2, 1],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [1, 1],\n",
    "    [1, 3],\n",
    "    [2, 2]\n",
    "])\n",
    "\n",
    "# Calcular H(X) y H(Y) usando la función de estimación de entropía\n",
    "H_X = entropy_estimation(X)\n",
    "H_Y = entropy_estimation(Y)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"La entropía estimada H(X) es: {H_X:.4f} bits\")\n",
    "print(f\"La entropía estimada H(Y) es: {H_Y:.4f} bits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ec026",
   "metadata": {},
   "source": [
    "## 2. Cálculo de entropía conjunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1ee5f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]),\n",
       " array([1, 2, 3]),\n",
       " array([[0.1, 0.2, 0.1],\n",
       "        [0.1, 0.2, 0.1],\n",
       "        [0. , 0. , 0.2]]),\n",
       " np.float64(1.5219280948873621),\n",
       " np.float64(1.5219280948873621),\n",
       " np.float64(2.7219280948873625))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculo_entropia_conjunta(realizaciones):\n",
    "    # ... (todo el código anterior se mantiene igual) ...\n",
    "    X = realizaciones[:,0]\n",
    "    Y = realizaciones[:,1]\n",
    "    alfabeto_x = np.unique(realizaciones[:,0])\n",
    "    alfabeto_y = np.unique(realizaciones[:,1])\n",
    "    tam_x = len(alfabeto_x)\n",
    "    tam_y = len(alfabeto_y)\n",
    "    matriz_frec = np.zeros((tam_x, tam_y), dtype=int)\n",
    "\n",
    "    for xi, yi in zip(X,Y):\n",
    "        i = np.where(alfabeto_x == xi)[0][0]\n",
    "        j = np.where(alfabeto_y == yi)[0][0]  \n",
    "        matriz_frec[i, j] += 1\n",
    "\n",
    "    matriz_prob = matriz_frec/len(realizaciones)\n",
    "    entropiaX = entropy_estimation(X)\n",
    "    entropiaY = entropy_estimation(Y)\n",
    "    entropiaConjunta = -np.sum(matriz_prob[matriz_prob > 0] * np.log2(matriz_prob[matriz_prob > 0]))\n",
    "\n",
    "    return alfabeto_x, alfabeto_y, matriz_prob, entropiaX, entropiaY, entropiaConjunta\n",
    "\n",
    "# Llamada a la función\n",
    "calculo_entropia_conjunta(realizaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77fcdb",
   "metadata": {},
   "source": [
    "### 2.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42714fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DEL ANÁLISIS DE ENTROPÍA CONJUNTA ===\n",
      "Alfabeto de X: [1 2 3]\n",
      "Alfabeto de Y: [1 2 3]\n",
      "\n",
      "Matriz de Probabilidad Conjunta P(X, Y):\n",
      "[[0.1 0.2 0.1]\n",
      " [0.1 0.2 0.1]\n",
      " [0.  0.  0.2]]\n",
      "\n",
      "Entropía H(X): 1.5219 bits\n",
      "Entropía H(Y): 1.5219 bits\n",
      "Entropía Conjunta H(X, Y): 2.7219 bits\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de la función y desempaquetado de resultados\n",
    "alfabeto_x, alfabeto_y, matriz_prob, entropiaX, entropiaY, entropiaConjunta = calculo_entropia_conjunta(realizaciones)\n",
    "\n",
    "# Visualización de los resultados obtenidos\n",
    "print(\"=== RESULTADOS DEL ANÁLISIS DE ENTROPÍA CONJUNTA ===\")\n",
    "print(f\"Alfabeto de X: {alfabeto_x}\")\n",
    "print(f\"Alfabeto de Y: {alfabeto_y}\")\n",
    "print(\"\\nMatriz de Probabilidad Conjunta P(X, Y):\")\n",
    "print(matriz_prob)\n",
    "print(f\"\\nEntropía H(X): {entropiaX:.4f} bits\")\n",
    "print(f\"Entropía H(Y): {entropiaY:.4f} bits\")\n",
    "print(f\"Entropía Conjunta H(X, Y): {entropiaConjunta:.4f} bits\")\n",
    "print(\"====================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e43d8",
   "metadata": {},
   "source": [
    "## 3. Cálculo de entropía condicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculo_entropia_condicional(realizaciones):\n",
    "    \"\"\"\n",
    "    Calcula el alfabeto de X e Y, las probabilidades condicionales y las entropías \n",
    "    condicionales de X dado Y y Y dado X utilizando la lógica de entropía conjunta.\n",
    "    \"\"\"\n",
    "    # 1. Obtener datos base llamando a la función de entropía conjunta anterior\n",
    "    alfabeto_x, alfabeto_y, matriz_prob, entropiaX, entropiaY, entropiaConjunta = calculo_entropia_conjunta(realizaciones)\n",
    "\n",
    "    # 2. Calcular las frecuencias (probabilidades) marginales de X e Y\n",
    "    # Sumar filas para p(x) y columnas para p(y)\n",
    "    p_X = np.sum(matriz_prob, axis=1)\n",
    "    p_Y = np.sum(matriz_prob, axis=0)\n",
    "\n",
    "    # 3. Inicializar matrices para probabilidades condicionales\n",
    "    X_Y = np.zeros(matriz_prob.shape)\n",
    "    Y_X = np.zeros(matriz_prob.shape)\n",
    "\n",
    "    # 4. Calcular las probabilidades condicionales P(X | Y) y P(Y | X)\n",
    "    # Se aplica la definición: P(A|B) = P(A,B) / P(B)\n",
    "    for i in range(matriz_prob.shape[0]):      # filas -> X\n",
    "        for j in range(matriz_prob.shape[1]):  # columnas -> Y\n",
    "            if p_Y[j] > 0:\n",
    "                X_Y[i, j] = matriz_prob[i, j] / p_Y[j]\n",
    "            if p_X[i] > 0:\n",
    "                Y_X[i, j] = matriz_prob[i, j] / p_X[i]\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "    # 5. Calcular las entropías condicionales H(X | Y) y H(Y | X)\n",
    "    # H(X|Y) = -Σ Σ p(x,y) * log2( p(x|y) )\n",
    "    # Usamos matriz_prob para la ponderación externa y X_Y/Y_X para el logaritmo\n",
    "    mask = matriz_prob > 0\n",
    "    entropia_X_dado_Y = -np.sum(matriz_prob[mask] * np.log2(X_Y[mask]))\n",
    "    entropia_Y_dado_X = -np.sum(matriz_prob[mask] * np.log2(Y_X[mask]))\n",
    "\n",
    "    return alfabeto_x, alfabeto_y, X_Y, Y_X, entropia_X_dado_Y, entropia_Y_dado_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f80bb",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41d94a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DEL ANÁLISIS DE ENTROPÍA CONDICIONAL ===\n",
      "H(X|Y) [Incertidumbre de X dado Y]: 1.2000 bits\n",
      "H(Y|X) [Incertidumbre de Y dado X]: 1.2000 bits\n",
      "\n",
      "Matriz de Probabilidad Condicional P(X|Y):\n",
      "[[0.5  0.5  0.25]\n",
      " [0.5  0.5  0.25]\n",
      " [0.   0.   0.5 ]]\n",
      "\n",
      "Matriz de Probabilidad Condicional P(Y|X):\n",
      "[[0.25 0.5  0.25]\n",
      " [0.25 0.5  0.25]\n",
      " [0.   0.   1.  ]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de la función y desempaquetado de resultados condicionales\n",
    "alfabeto_x, alfabeto_y, prob_XY, prob_YX, entr_XY, entr_YX = calculo_entropia_condicional(realizaciones)\n",
    "\n",
    "# Visualización de los resultados obtenidos\n",
    "print(\"=== RESULTADOS DEL ANÁLISIS DE ENTROPÍA CONDICIONAL ===\")\n",
    "print(f\"H(X|Y) [Incertidumbre de X dado Y]: {entr_XY:.4f} bits\")\n",
    "print(f\"H(Y|X) [Incertidumbre de Y dado X]: {entr_YX:.4f} bits\")\n",
    "\n",
    "print(\"\\nMatriz de Probabilidad Condicional P(X|Y):\")\n",
    "print(prob_XY)\n",
    "\n",
    "print(\"\\nMatriz de Probabilidad Condicional P(Y|X):\")\n",
    "print(prob_YX)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354b9f2",
   "metadata": {},
   "source": [
    "## 4. Cálculo información mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_informacion_mutua(realizaciones):\n",
    "    \"\"\"\n",
    "    Calcula la información mutua entre dos variables aleatorias discretas.\n",
    "    \n",
    "    Parámetros:\n",
    "        realizaciones (array): Matriz N x 2 con las realizaciones de dos variables aleatorias discretas.\n",
    "    \n",
    "    Retorno:\n",
    "        informacionMutua (float): Escalar que representa la información mutua entre las dos variables.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    alfabeto_x, alfabeto_y, matriz_prob, entropiaX, entropiaY, entropiaConjunta = calculo_entropia_conjunta(realizaciones)\n",
    "\n",
    "    _, _, prob_XY, prob_YX, entr_XY, entr_YX = calculo_entropia_conjunta(realizaciones)\n",
    "    \n",
    "    informacionMutua = entropiaX + entropiaY - entropiaConjunta\n",
    "    return informacionMutua\n",
    "\n",
    "calculo_informacion_mutua(realizaciones)\n",
    "\n",
    "\n",
    "def calculo_informacion_mutua(realizaciones):\n",
    "    \"\"\"\n",
    "    Calcula la información mutua entre dos variables aleatorias discretas\n",
    "    utilizando la relación: I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "    \"\"\"\n",
    "    # 1. Obtenemos las entropías marginales y la conjunta\n",
    "    _, _, _, hx, hy, h_conjunta = calculo_entropia_conjunta(realizaciones)\n",
    "    \n",
    "    # 2. La Información Mutua es la intersección de la información de X e Y\n",
    "    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "    informacionMutua = hx + hy - h_conjunta\n",
    "    \n",
    "    return informacionMutua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7b665",
   "metadata": {},
   "source": [
    "### 4.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d2ed93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.32192809488736174)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de uso con una matriz de realizaciones\n",
    "\n",
    "# Calcular la información mutua\n",
    "informacionMutua = calculo_informacion_mutua(realizaciones)\n",
    "informacionMutua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d380f64",
   "metadata": {},
   "source": [
    "## 5. Capacidad canal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad538f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def capacidad_canal_44(P):\n",
    "    \"\"\"\n",
    "    Calcula la capacidad de un canal de 4x4 y la distribución de entrada óptima\n",
    "    maximizando la Información Mutua I(X;Y) = H(Y) - H(Y|X).\n",
    "    \"\"\"\n",
    "    capacidad_canal = 0\n",
    "    pX_optima = np.zeros(4)\n",
    "    paso = 0.05\n",
    "    \n",
    "    # 1. Calcular la entropía condicional H(Y|X)\n",
    "    # Como H(Y|X) = sum_x p(x) H(Y|X=x), y H(Y|X=x) depende solo de las filas de P.\n",
    "    # Calculamos la entropía de cada fila de la matriz de transición\n",
    "    H_Y_dado_X_fijo = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        fila = P[i, :]\n",
    "        fila_valida = fila[fila > 0]\n",
    "        H_Y_dado_X_fijo[i] = -np.sum(fila_valida * np.log2(fila_valida))\n",
    "    \n",
    "    # 2. Recorrer todas las combinaciones posibles de pX para maximizar I(X;Y)\n",
    "    for p1 in np.arange(0, 1 + paso, paso):\n",
    "        for p2 in np.arange(0, 1 - p1 + paso, paso):\n",
    "            for p3 in np.arange(0, 1 - p1 - p2 + paso, paso):\n",
    "                p4 = 1 - p1 - p2 - p3\n",
    "                if p4 < -1e-9 or p4 > 1.0000001: # Tolerancia para errores de punto flotante\n",
    "                    continue\n",
    "                \n",
    "                # Vector de probabilidades de entrada p(X)\n",
    "                pX = np.array([p1, p2, p3, p4])\n",
    "                \n",
    "                # 3. Calcular la distribución de salida p(Y) = p(X) * P\n",
    "                # Esto es la suma p(y) = sum_x p(x) * P(y|x)\n",
    "                pY = np.dot(pX, P)\n",
    "                \n",
    "                # 4. Calcular la entropía de salida H(Y)\n",
    "                pY_valido = pY[pY > 0]\n",
    "                HY = -np.sum(pY_valido * np.log2(pY_valido))\n",
    "                \n",
    "                # 5. Calcular la entropía condicional total H(Y|X) = sum p(x)H(Y|X=x)\n",
    "                HYX = np.sum(pX * H_Y_dado_X_fijo)\n",
    "                \n",
    "                # 6. Calcular la información mutua I(X;Y)\n",
    "                informacion_mutua = HY - HYX\n",
    "                \n",
    "                # 7. Actualizar si encontramos un máximo (Capacidad = max I(X;Y))\n",
    "                if informacion_mutua > capacidad_canal:\n",
    "                    capacidad_canal = informacion_mutua\n",
    "                    pX_optima = pX\n",
    "\n",
    "    return capacidad_canal, pX_optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f01498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS DE CAPACIDAD DE CANAL 4x4 ===\n",
      "\n",
      "Matriz de Canal Normalizada P(Y|X):\n",
      "[[0.         1.         0.         0.        ]\n",
      " [0.45454545 0.09090909 0.         0.45454545]\n",
      " [0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.11764706 0.70588235 0.17647059]]\n",
      "\n",
      "Capacidad del Canal (C): 1.0948 bits/símbolo\n",
      "Distribución de Entrada Óptima p(X): [0.4  0.25 0.1  0.25]\n",
      "Suma de p(X) óptima: 1.00\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso con una matriz de canal 4x4\n",
    "P = np.array([\n",
    "    [0, 2, 0, 0],\n",
    "    [5, 1, 0, 5],\n",
    "    [14, 7, 0, 0],\n",
    "    [0, 2, 12, 3]\n",
    "])\n",
    "# 1. Normalización de la matriz de canal P proporcionada\n",
    "# Dividimos cada fila por su suma para que sumen 1 (P(Y|X) válida)\n",
    "P_norm = P / P.sum(axis=1, keepdims=True)\n",
    "\n",
    "# 2. Ejecución de mi función de capacidad\n",
    "capacidad, pX_opt = capacidad_canal_44(P_norm)\n",
    "\n",
    "# 3. Visualización de resultados\n",
    "print(\"=== ANÁLISIS DE CAPACIDAD DE CANAL 4x4 ===\")\n",
    "print(\"\\nMatriz de Canal Normalizada P(Y|X):\")\n",
    "print(P_norm)\n",
    "print(f\"\\nCapacidad del Canal (C): {capacidad:.4f} bits/símbolo\")\n",
    "print(f\"Distribución de Entrada Óptima p(X): {pX_opt}\")\n",
    "print(f\"Suma de p(X) óptima: {np.sum(pX_opt):.2f}\")\n",
    "print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad5f62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS DE INFORMACIÓN DEL TEXTO ===\n",
      "1. Entropía de Palabras (H_p): 4.6202 bits/palabra\n",
      "2. Entropía de Caracteres (H_c): 4.0378 bits/símbolo\n",
      "3. Redundancia estimada: 16.01%\n",
      "4. Entropía Conjunta H(X,Y) entre letras: 6.2876 bits\n",
      "5. Entropía Condicional H(X|Y): 2.2484 bits\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# ejemplo texto\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preparación del Texto (Limpieza de \"ruido\" inicial)\n",
    "texto_quijote = \"\"\"En un lugar de la Mancha2, de cuyo nombre no quiero acordarme3, \n",
    "no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, \n",
    "adarga antigua, rocín flaco y galgo corredor4...\"\"\"\n",
    "\n",
    "\n",
    "# Normalización: minúsculas y solo caracteres alfabéticos (Filtro de \"suavizado\")\n",
    "texto_limpio = \"\".join([c.lower() for c in texto_quijote if c.isalpha() or c.isspace()])\n",
    "\n",
    "# --- CÁLCULO 1: Entropía de Palabras (Riqueza de la fuente) ---\n",
    "palabras = texto_limpio.split()\n",
    "H_palabras = entropy_estimation(palabras)\n",
    "\n",
    "# --- CÁLCULO 2: Redundancia del Lenguaje ---\n",
    "# Calculamos la entropía máxima posible para el alfabeto español (aprox. 27 letras + espacio)\n",
    "H_max = np.log2(28) \n",
    "H_caracteres = entropy_estimation(list(texto_limpio))\n",
    "redundancia = 1 - (H_caracteres / H_max)\n",
    "\n",
    "# --- CÁLCULO 3: Dependencia entre Letras (Entropía Conjunta y Condicional) ---\n",
    "# Creamos pares de letras consecutivas (Letra actual X, Letra siguiente Y)\n",
    "X = list(texto_limpio[:-1])\n",
    "Y = list(texto_limpio[1:])\n",
    "realizaciones_texto = np.array(list(zip(X, Y)))\n",
    "\n",
    "#  relación entre caracteres adyacentes\n",
    "alf_x, alf_y, m_prob, hx, hy, h_conjunta = calculo_entropia_conjunta(realizaciones_texto)\n",
    "_, _, _, _, h_x_dado_y, _ = calculo_entropia_condicional(realizaciones_texto)\n",
    "\n",
    "# --- VISUALIZACIÓN DE RESULTADOS ---\n",
    "print(\"=== ANÁLISIS DE INFORMACIÓN DEL TEXTO ===\")\n",
    "print(f\"1. Entropía de Palabras (H_p): {H_palabras:.4f} bits/palabra\")\n",
    "print(f\"2. Entropía de Caracteres (H_c): {H_caracteres:.4f} bits/símbolo\")\n",
    "print(f\"3. Redundancia estimada: {redundancia*100:.2f}%\")\n",
    "print(f\"4. Entropía Conjunta H(X,Y) entre letras: {h_conjunta:.4f} bits\")\n",
    "print(f\"5. Entropía Condicional H(X|Y): {h_x_dado_y:.4f} bits\")\n",
    "print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35d719",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1. Entropía de Palabras ($H_p = 4.6202$ bits/palabra)\n",
    "Este valor representa la incertidumbre promedio por cada palabra del texto.\n",
    "* **Significado**: Al ser mayor que la de caracteres, confirma que las palabras como unidades de información poseen una \"energía\" informativa más alta.\n",
    "* **Relación con señales**: Equivale a analizar los picos de mayor magnitud en un espectro; las palabras son los eventos que realmente definen el contenido del mensaje.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Entropía de Caracteres ($H_c = 4.0378$ bits/símbolo)\n",
    "Es la cantidad de información promedio que aporta cada letra individualmente.\n",
    "* **Interpretación**: Un valor de ~4 bits es estándar para el español normalizado. \n",
    "* **Importancia del preprocesamiento**: Este valor es relativamente bajo gracias a que eliminamos el \"ruido\" (números y puntuación), lo que permite que la distribución de probabilidades sea más limpia y refleje la estructura del idioma.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Redundancia Estimada ($16.01\\%$)\n",
    "Indica que el $16\\%$ del texto no aporta información nueva, sino que sirve para dar estructura.\n",
    "* **Análisis**: Esta redundancia actúa como un \"margen de seguridad\". Es lo que permite que, si el texto sufre interferencias (como el ruido gaussiano o sal y pimienta que vimos en imágenes), todavía seamos capaces de reconstruir el mensaje.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Entropía Conjunta ($H(X,Y) = 6.2876$ bits)\n",
    "Es la incertidumbre total de observar pares de letras consecutivas.\n",
    "* **Observación clave**: Observo que $H(X,Y) < H(X) + H(Y)$ (ya que $4.03 + 4.03 = 8.06$).\n",
    "* **Conclusión**: Esta diferencia confirma que existe una fuerte dependencia entre letras adyacentes. Las letras no aparecen al azar, sino siguiendo patrones, similar a cómo los píxeles de una imagen original guardan correlación con sus vecinos antes de ser corrompidos por ruido.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Entropía Condicional ($H(X|Y) = 2.2484$ bits)\n",
    "Representa la incertidumbre que queda sobre una letra si ya conocemos la anterior.\n",
    "* **Resultado**: Al ser significativamente menor que la entropía individual ($2.24 < 4.03$), se demuestra que el contexto reduce la incertidumbre a casi la mitad.\n",
    "* **Analogía con filtrado**: Conocer la letra anterior actúa como un \"filtro predictor\". Si la señal es predecible, el error de estimación (la incertidumbre residual) disminuye drásticamente, permitiendo una transmisión más eficiente por el canal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f3d32",
   "metadata": {},
   "source": [
    "# PRACTICA 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dfe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía de la Entrada H(X): 1.9998 bits\n",
      "Entropía de la Salida H(Y): 1.9998 bits\n",
      "Incertidumbre que queda H(X|Y): 1.5845 bits\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "# 1. Cargar los archivos .mat\n",
    "# Estos archivos contienen las realizaciones de nuestras variables X e Y\n",
    "data_x = scipy.io.loadmat('VariableX.mat')\n",
    "data_xy = scipy.io.loadmat('VariableX_Y.mat')\n",
    "\n",
    "\n",
    "# Extraigo los números reales del diccionario\n",
    "muestras_x = data_x['X']\n",
    "\n",
    "# Lo convierto en un vector simple para mi función de entropía\n",
    "vector_x = muestras_x.flatten() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Extraer la matriz conjunta del diccionario\n",
    "realizaciones_conjuntas = data_xy['XY']\n",
    "\n",
    "# 2. Separar las columnas para cálculos individuales\n",
    "X_desde_canal = realizaciones_conjuntas[:, 0]\n",
    "Y_desde_canal = realizaciones_conjuntas[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "500c34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANÁLISIS COMPARATIVO DE ENTROPÍA ---\n",
      "H(X) Fuente Referencia (N=5000):  1.7703 bits\n",
      "H(X) Entrada del Canal (N=10000): 1.9998 bits\n",
      "\n",
      "--- RESULTADOS DEL CANAL X -> Y ---\n",
      "Entropía de la Salida H(Y):       1.9998 bits\n"
     ]
    }
   ],
   "source": [
    "h_x_referencia = entropy_estimation(vector_x)\n",
    "\n",
    "# =================================================================\n",
    "# 3. PROCESAMIENTO CASO B: CANAL DE COMUNICACIÓN (10.000 muestras)\n",
    "# =================================================================\n",
    "# Extraemos la matriz conjunta (Col 0: Entrada X, Col 1: Salida Y)\n",
    "realizaciones_conjuntas = data_xy['XY']\n",
    "\n",
    "X_canal = realizaciones_conjuntas[:, 0]\n",
    "Y_canal = realizaciones_conjuntas[:, 1]\n",
    "\n",
    "# Calculamos entropías individuales del sistema del canal\n",
    "h_x_canal = entropy_estimation(X_canal)\n",
    "h_y_canal = entropy_estimation(Y_canal)\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- ANÁLISIS COMPARATIVO DE ENTROPÍA ---\")\n",
    "print(f\"H(X) Fuente Referencia (N=5000):  {h_x_referencia:.4f} bits\")\n",
    "print(f\"H(X) Entrada del Canal (N=10000): {h_x_canal:.4f} bits\")\n",
    "\n",
    "\n",
    "print(\"\\n--- RESULTADOS DEL CANAL X -> Y ---\")\n",
    "print(f\"Entropía de la Salida H(Y):       {h_y_canal:.4f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DE ENTROPÍA CONJUNTA H(X,Y) ===\n",
      "Alfabeto de la Entrada (X): [0 1 2 3]\n",
      "Alfabeto de la Salida  (Y): [0 1 2 3]\n",
      "\n",
      "Entropía Conjunta H(X,Y): 3.5843 bits\n",
      "Suma de entropías marginales H(X)+H(Y): 3.9996 bits\n",
      "Diferencia (Información compartida): 0.4153 bits\n",
      "==============================================\n",
      "\n",
      "Matriz de Probabilidad Conjunta P(X,Y):\n",
      "[[0.     0.0839 0.0781 0.0825]\n",
      " [0.0865 0.     0.0794 0.0835]\n",
      " [0.083  0.0854 0.     0.082 ]\n",
      " [0.0863 0.0828 0.0866 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "## enttropia conjunta\n",
    "realizaciones_conjuntas = data_xy['XY']\n",
    "\n",
    "# 2. Ejecución del cálculo de entropía conjunta\n",
    "# Nota: h_x e h_y devueltos aquí corresponden a las 10.000 muestras\n",
    "alf_x, alf_y, m_prob, h_x_c, h_y_c, h_conjunta = calculo_entropia_conjunta(realizaciones_conjuntas)\n",
    "\n",
    "# 3. Visualización de los resultados de la unión de X e Y\n",
    "print(\"=== RESULTADOS DE ENTROPÍA CONJUNTA H(X,Y) ===\")\n",
    "print(f\"Alfabeto de la Entrada (X): {alf_x}\")\n",
    "print(f\"Alfabeto de la Salida  (Y): {alf_y}\")\n",
    "print(f\"\\nEntropía Conjunta H(X,Y): {h_conjunta:.4f} bits\")\n",
    "\n",
    "# 4. Verificación de la relación de independencia\n",
    "# Si fueran independientes, H(X,Y) sería igual a H(X) + H(Y)\n",
    "h_suma_marginal = h_x_c + h_y_c\n",
    "print(f\"Suma de entropías marginales H(X)+H(Y): {h_suma_marginal:.4f} bits\")\n",
    "print(f\"Diferencia (Información compartida): {h_suma_marginal - h_conjunta:.4f} bits\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "print(\"\\nMatriz de Probabilidad Conjunta P(X,Y):\")\n",
    "print(m_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fdd8206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DE ENTROPÍA CONDICIONAL ===\n",
      "H(X|Y) [Incertidumbre de X tras observar Y]: 1.5845 bits\n",
      "H(Y|X) [Incertidumbre de Y tras conocer X]:  1.5844 bits\n",
      "\n",
      "--- ANÁLISIS DEL CANAL ---\n",
      "El canal introduce ruido: H(Y|X) = 1.5844 bits.\n",
      "Existe pérdida de información (equivocación): H(X|Y) = 1.5845 bits.\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# CONDICIONAL\n",
    "# =================================================================\n",
    "# CÁLCULO ESPECÍFICO DE ENTROPÍA CONDICIONAL (Fichero VariableX_Y.mat)\n",
    "# =================================================================\n",
    "\n",
    "# 1. Recuperamos la matriz de realizaciones (10.000 muestras)\n",
    "realizaciones_conjuntas = data_xy['XY']\n",
    "\n",
    "# 2. Ejecución del cálculo de entropía condicional\n",
    "# Obtenemos las matrices de probabilidad condicional y las entropías resultantes\n",
    "alf_x, alf_y, p_X_dado_Y, p_Y_dado_X, h_x_dado_y, h_y_dado_x = calculo_entropia_condicional(realizaciones_conjuntas)\n",
    "\n",
    "# 3. Visualización de los resultados de incertidumbre residual\n",
    "print(\"=== RESULTADOS DE ENTROPÍA CONDICIONAL ===\")\n",
    "print(f\"H(X|Y) [Incertidumbre de X tras observar Y]: {h_x_dado_y:.4f} bits\")\n",
    "print(f\"H(Y|X) [Incertidumbre de Y tras conocer X]:  {h_y_dado_x:.4f} bits\")\n",
    "\n",
    "# 4. Interpretación técnica\n",
    "print(\"\\n--- ANÁLISIS DEL CANAL ---\")\n",
    "if h_y_dado_x > 0.1:\n",
    "    print(f\"El canal introduce ruido: H(Y|X) = {h_y_dado_x:.4f} bits.\")\n",
    "else:\n",
    "    print(\"El canal es casi ideal (muy poco ruido).\")\n",
    "\n",
    "if h_x_dado_y > 0:\n",
    "    print(f\"Existe pérdida de información (equivocación): H(X|Y) = {h_x_dado_y:.4f} bits.\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Opcional: Mostrar una de las matrices condicionales\n",
    "# print(\"\\nMatriz de Probabilidad Condicional P(Y|X):\")\n",
    "# print(p_Y_dado_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57bacb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADO DE INFORMACIÓN MUTUA I(X;Y) ===\n",
      "Información Mutua: 0.4153 bits\n",
      "Eficiencia de la transmisión: 20.77%\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "realizaciones_conjuntas = data_xy['XY']\n",
    "i_mutua = calculo_informacion_mutua(realizaciones_conjuntas)\n",
    "\n",
    "# --- VISUALIZACIÓN DE RESULTADOS ---\n",
    "print(\"=== RESULTADO DE INFORMACIÓN MUTUA I(X;Y) ===\")\n",
    "print(f\"Información Mutua: {i_mutua:.4f} bits\")\n",
    "\n",
    "# Cálculo de eficiencia (opcional para el examen)\n",
    "# ¿Qué porcentaje de la información de la entrada X llega realmente a Y?\n",
    "_, _, _, hx_c, _, _ = calculo_entropia_conjunta(realizaciones_conjuntas)\n",
    "eficiencia = (i_mutua / hx_c) * 100\n",
    "\n",
    "print(f\"Eficiencia de la transmisión: {eficiencia:.2f}%\")\n",
    "print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e360b",
   "metadata": {},
   "source": [
    "Es fundamental recordar que la capacidad es el valor máximo teórico de la información mutua. Mientras que calculo_informacion_mutua nos da la información que atraviesa el canal con los datos que ya tenemos (los 10.000 datos del fichero), la función capacidad_canal_44 nos dice lo mejor que podría llegar a funcionar ese canal si optimizáramos la fuente de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54e3cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS DE CAPACIDAD DEL CANAL ===\n",
      "Capacidad Máxima (C): 0.4154 bits/símbolo\n",
      "Distribución óptima de entrada p(X): [0.25 0.25 0.25 0.25]\n",
      "\n",
      "Información Mutua actual I(X;Y): 0.4153 bits\n",
      "Diferencia (Margen de mejora): 0.0001 bits\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# CÁLCULO DE LA CAPACIDAD DEL CANAL (Optimización de la fuente)\n",
    "# =================================================================\n",
    "\n",
    "# 1. Obtenemos la matriz de probabilidad condicional P(Y|X) \n",
    "# Esta matriz es la que define \"cómo es el canal\" físicamente.\n",
    "# La obtenemos de nuestra función de entropía condicional previa.\n",
    "_, _, _, P_transicion, _, _ = calculo_entropia_condicional(realizaciones_conjuntas)\n",
    "\n",
    "# 2. Calculamos la capacidad y la distribución de entrada óptima\n",
    "capacidad, pX_opt = capacidad_canal_44(P_transicion)\n",
    "\n",
    "# 3. Visualización de resultados de capacidad\n",
    "print(\"=== ANÁLISIS DE CAPACIDAD DEL CANAL ===\")\n",
    "print(f\"Capacidad Máxima (C): {capacidad:.4f} bits/símbolo\")\n",
    "print(f\"Distribución óptima de entrada p(X): {pX_opt}\")\n",
    "\n",
    "# 4. Comparación con la Información Mutua actual\n",
    "i_mutua_actual = calculo_informacion_mutua(realizaciones_conjuntas)\n",
    "print(f\"\\nInformación Mutua actual I(X;Y): {i_mutua_actual:.4f} bits\")\n",
    "print(f\"Diferencia (Margen de mejora): {capacidad - i_mutua_actual:.4f} bits\")\n",
    "print(\"========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Practicas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
